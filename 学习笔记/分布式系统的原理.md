走向分布式

走向分布式
作者：ccshih
来源：http://ithelp.ithome.com.tw/profile/share?id=20060041&page=1
整理：邓草原 1

1.scalability

处理单机处理不过来的数据，存储单机存不过来的数据，计算（map-reduce），存储（hadoop data system），分库分表

2.主要的技术手段是 partition 和 replication
partition把一整块数据分成几个块

replication要做的是增加副本，提高冗余

3.PARTITION
Round-Robin: 资料轮流进多台机器。好处是 load balance，坏处是不适合有 session
或资料相依性 (need join) 的应用。变型是可以用 thread pool，每个机器固定配几个
thread，这可以避免某个运算耗时过久，而档到後面运算的问题。
• Range: 事先定好每台机器的防守范围，如 key 在 1~1000 到 A 机器。优点是简单，
只需要维护一些 metadata。问题是弹性较差，且会有 hotspot 的问题 (大量资料数值
都集中在某个范围)。MongoDB 在早期版本只支援这种切割方式。
• Hash: 用 Hash 来决定资料要在哪台机器上。简单的 Hash 像是取馀数，但取馀数在
新增机器时会有资料迁移的问题，所以现在大家比较常用 Consistent Hashing 来避
免这个问题。Hash 可以很平均的分布，且只需要非常少的 metadata。但 Hash 规则
不好掌握，比方说我们就很难透过自定 Hash 规则让某几笔资料一定要在一起。大部
分的 Data Store 都是采用 Consistent Hashing。
• Manual: 手动建一个对照表，优点是想要怎麽分配都可以，缺点是要自己控制资料和
负载的均衡，且会有大量 metadata 要维护。

4.什么时候不要map-reduce
如果 query 很耗时，那分散的确会比较好；但如果 query 很快 (比方有
用到 DB 的 index lookup)，那分散会增加效能降低的风险。
Hadoop 的 Map Reduce 就是透过分散提升效率，因为有很多资料要扫，所以分散是值得
的。在这种状况下，效能的增加盖过效能降低的风险

5.METADATA 管理

有些切割方式，像是 Hashing 的 metadata 量非常少，这是相对容易管的。但有些切割方
式有很多 metadata，且有些方式在每次 insert 都要更新 metadata (bad practice~)，那这
就麻烦了。
一个最简单的方式就是有一台机器专门管这些 metadata (meta server, config server...)，
若需要 metadata 就来这边问。但明显的这会有单点问题。
现在常见的解法是用 Apache Zookeeper (ZK)，这是一个维持 cluster 中共同状态的分散
式系统，透过 ZK 来维护这些 metadata 是许多分散式系统的普遍作法。ZK 有自己的 HA
和 consistency 机制可以保障资料，而且在 production 环境中一次要用 2n+1 (n>0) 个节
点 (minumum = 3)，只要不要大於 n 个节点挂掉都可以正常服务。因为 ZK 里的资料
很重要！

6. REPLICATION
资料复制是维持可用性的方法，因为资料复制好几份到不同机器，所以只要有一台机器还
在，资料就拿的到。
但只要有资料复制，就一定会有延迟的状况，也就是在资料复制完成前，多台机器的资料
是不一致的。
有的系统对於资料一致性读很要求，就会采同步复制，要复制完成後资料写入才会完成。
但这样会很慢，尤其是副本越来越多的时候。
所以比较有效率的作法是非同步复制，但一定会有一段时间不一致。

7.无强一致性及无法决定执行顺序带来的问题
假设有两个节点，两个不同机器的节点已经同步好帐户馀额 =15 元，有两个客户，一个要
提款 10 元，一个要提 15 元，但是发到不同的机器上处理。两个节点都以为都会以为没问
题 (馀额 >=0)，但实际上需要拒绝其中一个请求。
为了避免这种问题，许多资料系统都只允许在一个节点上写资料。如果有拆 partition，那
每个 partition 内会只有一个 master 及零至多个 slave(replica)，只有 master 能写资料。再
极端一点，有些资料系统为了避免读到不一致的资料，甚至会只允许在 master 上读资料，
那这样 replica 就完全只是备援的角色，没有分散读取的能力。

8.最终一致性
最终一致性是说，只要资料不再更新，终有个时刻，所有节点会协调出一个一致的状态。
这听起来相当的不可靠啊 XD
这的确是很不可靠。之前提到的执行顺序的问题，许多系统是利用 Vector clock，透过讯
息传递来归纳出执行的时序。但由於 Vector clock 需要透过讯息交换 logical timestamp，
才能整理出时序。所以如果有节点很孤僻不常跟其他人讲话，那推敲出来的时序就不精准。
实际上，Vector clock 不保证能推测出完整时序 (total order)，只能推测出部分时序 (partial
order)，也就是可能只能推敲出类似这样的结果：A < D, B < D, C < D，那你说 A, B, C 的
顺序呢？很抱歉，因为只有部分时序，所以不知道

9.CAP

CAP Theorem 的 CAP 分别是指：
• C (Strong Consistency): 在任何时候，从丛集中的任两个节点得到的状态都是一样
的。
• A (Availability): 若一个节点没有坏掉，那它就必须要能正常服务。
• P (Partition Tolerance): 若一个丛集因网路问题或节点故障问题，被切割成两个 (或
以上) 不完整的 sub cluster 时，系统整体还能正常运作。
分散式系统中，这三个特性至多只有两个能同时存在。



